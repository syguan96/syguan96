<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Shanyan Guan - Academic Homepage</title>
    <link rel="stylesheet" href="styles.css">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&display=swap" rel="stylesheet">
</head>
<body>
    <div class="container">
        <!-- Personal Information Header -->
        <header class="header">
            <div class="header-content">
                <div class="profile-info">
                    <h1 class="name">官善琰</h1>
                    <p class="title">Researcher | vivo Imaging Department</p>
                    <div class="contact-info">
                        <p><strong>Email:</strong> guanshanyan@vivo.com</p>
                        <p><strong>Address:</strong> vivo Shanghai R&D Center, Pudong New Area, Shanghai</p>
                        <div class="recruitment-info">
                            <p><span class="highlight">长期招收实习生</span>，<span class="highlight">少量应届校招名额</span>。研究方向：<span class="highlight">视频模型后训练、高效图像生成/编辑、世界模型</span>等。有意者请<span class="email">发邮件联系我</span>并附上简历。</p>
                        </div>
                    </div>
                </div>
                <div class="profile-image">
                    <img src="photo.jpg" alt="Profile Photo">
                </div>
            </div>
        </header>

        

        <!-- About Me -->
        <section class="research-intro">
            <h2>About Me</h2>
            <p>
                I am Shanyan Guan (官善琰), a researcher specializing in computer vision and artificial intelligence. Currently, I work at vivo Imaging Department, focusing on AI Photography R&D. I received my Ph.D. from Shanghai Jiao Tong University under the supervision of Prof. Xiaokang Yang and Prof. Yunbo Wang, and my B.S. from Xidian University in 2017.
            </p>
            <p>
                <strong>Industry Experience:</strong> I have gained valuable experience through internships at leading technology companies including Tencent XR, SenseTime, Tencent YouTu Lab, and miHoYo, where I applied computer vision and AI technologies across diverse domains including gaming, autonomous systems, and mobile applications.
            </p>
            <p>
                <strong>Research Focus:</strong> My research spans both academic and industrial applications. During my Ph.D., I focused on machine understanding of real-world dynamics and motion analysis. Currently, I concentrate on high-resolution image generation, video synthesis, and world models for mobile camera systems.
            </p>
        </section>

        <!-- Publications -->
        <section class="publications">
            <h2>Publications <span class="legend">(<sup>*</sup> Co-first author, <sup>†</sup> Project Lead)</span></h2>
            <div class="publication-item">
                <h3>Describe, Don't Dictate: Semantic Image Editing with Natural Language Intent</h3>
                <p class="authors">En Ci<sup>*</sup>, <strong>Shanyan Guan</strong><sup>*</sup>, Yanhao Ge, Yilin Zhang, Wei Li, Zhenyu Zhang, Jian Yang, Ying Tai</p>
                <p class="venue"><strong>ICCV 2025</strong></p>
                <div class="links">
                    <a href="https://arxiv.org/abs/2508.20505" class="link">[Paper]</a>
                    <a href="https://zhuanlan.zhihu.com/p/1945784172087051421" class="link">[知乎]</a>
                </div>
            </div>
            <div class="publication-item">
                <h3>PostEdit: Posterior Sampling for Efficient Zero-Shot Image Editing</h3>
                <p class="authors">Feng Tian, Yixuan Li, Yichao Yan, <strong>Shanyan Guan</strong>, Yanhao Ge, Xiaokang Yang</p>
                <p class="venue"><strong>ICLR 2025</strong></p>
                <div class="links">
                    <a href="https://openreview.net/forum?id=J8YWCBPgx7" class="link">[Paper]</a>
                    <a href="https://github.com/TianFeng-AI/PostEdit" class="link">[Code]</a>
                </div>
            </div>
            <div class="publication-item">
                <h3>NeuMA: Neural Material Adaptor for Visual Grounding of Intrinsic Dynamics</h3>
                <p class="authors">Junyi Cao, <strong>Shanyan Guan</strong><sup>†</sup>, Yanhao Ge, Wei Li, Xiaokang Yang, Chao Ma</p>
                <p class="venue"><strong>NeurIPS 2024</strong></p>
                <div class="links">
                    <a href="https://openreview.net/pdf?id=AvWB40qXZh" class="link">[Paper]</a>
                    <a href="https://github.com/XJay18/NeuMA" class="link">[Code]</a>
                    <a href="https://xjay18.github.io/NeuMA_page/" class="link">[Project Page]</a>
                    <a href="https://zhuanlan.zhihu.com/p/5028868071" class="link">[知乎]</a>
                </div>
            </div>
            <div class="publication-item">
                <h3>HybridBooth: Hybrid Prompt Inversion for Efficient Subject-Driven Generation</h3>
                <p class="authors"><strong>Shanyan Guan<sup>*</sup></strong>, Yanhao Ge<sup>*</sup>, Ying Tai, Jian Yang, Wei Li, Mingyu You</p>
                <p class="venue"><strong>ECCV 2024</strong></p>
                <div class="links">
                    <a href="https://arxiv.org/abs/2410.08192" class="link">[Paper]</a>
                    <a href="https://sites.google.com/view/hybridbooth" class="link">[Project Page]</a>
                </div>
            </div>
            <div class="publication-item">
                <h3>CageNeRF: Cage-based Neural Radiance Field for Generalized 3D Deformation and Animation</h3>
                <p class="authors">Yicong Peng, Yichao Yan, Shengqi Liu, Yuhao Cheng, <strong>Shanyan Guan</strong>, Bowen Pan, Guangtao Zhai, Xiaokang Yang</p>
                <p class="venue"><strong>NeurIPS 2022</strong></p>
                <div class="links">
                    <a href="https://openreview.net/forum?id=kUnHCGiILeU" class="link">[Paper]</a>
                    <a href="https://github.com/PengYicong/CageNeRF" class="link">[Project Page]</a>
                    <a href="https://github.com/PengYicong/CageNeRF" class="link">[Code]</a>
                    <a href="https://zhuanlan.zhihu.com/p/581808674" class="link">[知乎]</a>
                </div>
            </div>
            <div class="publication-item">
                <h3>Out-of-Domain Human Mesh Reconstruction via Dynamic Bilevel Online Adaptation</h3>
                <p class="authors"><strong>Shanyan Guan</strong>, Jingwei Xu, Michelle Z He, Yunbo Wang, Bingbing Ni, Xiaokang Yang</p>
                <p class="venue"><strong>T-PAMI 2022</strong></p>
                <div class="links">
                    <a href="https://ieeexplore.ieee.org/document/9842366/" class="link">[Paper]</a>
                    <a href="https://sites.google.com/view/dynaboa" class="link">[Project Page]</a>
                    <a href="https://github.com/syguan96/DynaBOA" class="link">[Code]</a>
                </div>
            </div>
            <div class="publication-item">
                <h3>PTSEFormer: Progressive Temporal-Spatial Enhanced Transformer Towards Video Object Detection</h3>
                <p class="authors">Han Wang, Jun Tang, Xiaodong Liu, <strong>Shanyan Guan</strong>, Rong Xie, Li Song</p>
                <p class="venue"><strong>ECCV 2022</strong></p>
                <div class="links">
                    <a href="https://arxiv.org/abs/2209.02242" class="link">[Paper]</a>
                    <a href="https://github.com/syguan96/PTSEFormer" class="link">[Code]</a>
                </div>
            </div>
            <div class="publication-item">
                <h3>NeuroFluid: Fluid Dynamics Grounding with Particle-Driven Neural Radiance Fields</h3>
                <p class="authors"><strong>Shanyan Guan</strong>, Huayu Deng, Yunbo Wang, Xiaokang Yang</p>
                <p class="venue"><strong>ICML 2022</strong></p>
                <div class="links">
                    <a href="https://arxiv.org/abs/2203.01762" class="link">[Paper]</a>
                    <a href="https://syguan96.github.io/NeuroFluid/" class="link">[Project Page]</a>
                    <a href="https://github.com/syguan96/NeuroFluid" class="link">[Code]</a>
                    <a href="https://news.sjtu.edu.cn/jdzh/20220602/171985.html" class="link">[上海交大新闻网]</a>
                </div>
            </div>
            <div class="publication-item">
                <h3>Bilevel Online Adaptation for Out-of-Domain Human Mesh Reconstruction</h3>
                <p class="authors"><strong>Shanyan Guan</strong><sup>*</sup>, Jingwei Xu<sup>*</sup>, Yunbo Wang, Bingbing Ni, Xiaokang Yang</p>
                <p class="venue"><strong>CVPR 2021</strong></p>
                <div class="links">
                    <a href="https://arxiv.org/pdf/2103.16449" class="link">[Paper]</a>
                    <a href="https://sites.google.com/view/dynaboa/boa-cvpr-2021" class="link">[Project Page]</a>
                    <a href="https://github.com/syguan96/BOA" class="link">[Code]</a>
                </div>
            </div>
            <div class="publication-item">
                <h3>Collaborative Learning for Faster StyleGAN Embedding</h3>
                <p class="authors"><strong>Shanyan Guan</strong>, Ying Tai, Bingbing Ni, Feida Zhu, Feiyue Huang, Xiaokang Yang</p>
                <p class="venue"><strong>arXiv preprint, 2019</strong></p>
                <div class="links">
                    <a href="https://arxiv.org/abs/2007.01758" class="link">[Paper]</a>
                </div>
            </div>
            <div class="publication-item">
                <h3>Human Action Transfer Based on 3D Model Reconstruction</h3>
                <p class="authors"><strong>Shanyan Guan</strong><sup>*</sup>, Shuo Wen<sup>*</sup>, Dexin Yang<sup>*</sup>, Bingbing Ni, Wendong Zhang, Jun Tang, Xiaokang Yang</p>
                <p class="venue"><strong>AAAI'19 Oral</strong></p>
                <div class="links">
                    <a href="https://ojs.aaai.org/index.php/AAAI/article/view/4849" class="link">[Paper]</a>
                </div>
            </div>
        </section>

        <!-- Talks -->
        <section class="talks">
            <h2>Talks & Presentations</h2>
            <div class="talk-item">
                <h3>Next-Gen Camera Systems: Instilling Intuitive Imagination in Smartphones</h3>
                <div class="links">
                    <a href="https://ijcai24.org/industry-day/index.html" class="link">[Event Page]</a>
                </div>
            </div>
            <div class="talk-item">
                <h3>GAMES Webinar 337: Next-Gen Camera System: Instilling Intuitive Imagination in Smartphones</h3>
                <div class="links">
                    <a href="https://www.bilibili.com/video/BV1hM4m117mQ/" class="link">[Video]</a>
                </div>
            </div>
            <div class="talk-item">
                <h3>基于视觉的领域泛化和物理运动推断</h3>
                <div class="links">
                    <a href="https://course.zhidx.com/c/MTRhYzAyNWMwN2EzYTAwZTk5NjM=" class="link">[Course]</a>
                </div>
            </div>
            <div class="talk-item">
                <h3>基于视觉的物理规律反演</h3>
                <div class="links">
                    <a href="https://course.zhidx.com/teacher/detail/YjliMTY0MmZjNzY4YTEzNmUxNjM=" class="link">[Course]</a>
                </div>
            </div>
        </section>
    </div>
    <script src="script.js"></script>
</body>
</html>